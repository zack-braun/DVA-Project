{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# We'll use boston housing data as an example\n",
    "bh = load_boston()\n",
    "target = bh.target\n",
    "x_data = bh.data\n",
    "\n",
    "# We will evaluate the models using mean squared error\n",
    "# Since Python's Sklearn requires scorer to have greater is better property\n",
    "# We'll use the negated mean squared error as our scorer using make_scorer function\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "#Where applicable we'll use the same random seed\n",
    "rand_state = 748932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of the models require that the input data is standardized. For these models we will use sklearn's prediction pipeline capabilities to ensure the standardization occurs as part of our cross-validated model tuning. For the remaining models we will use the un-standardized features in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters searched are: {'clf__C': 10, 'clf__epsilon': 0.9, 'clf__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "predPipeSVR = Pipeline(steps = [('scale', StandardScaler()), ('clf', SVR())])\n",
    "\n",
    "# We need to tune the parameters c and kernel options:{'linear', 'poly', 'rbf', 'sigmoid'}\n",
    "parameters = {'clf__kernel':('linear', 'rbf', 'poly', 'sigmoid'), \n",
    "              'clf__C':[0.001, 0.01, 0.1, 1, 10, 100], \n",
    "              'clf__epsilon':[0.1, 0.3, 0.5, 0.9]}\n",
    "\n",
    "clf = GridSearchCV(predPipeSVR, parameters, scoring = scorer, cv=5, n_jobs = -1)\n",
    "clf.fit(x_data, target)\n",
    "resultsSVR = clf.cv_results_\n",
    "\n",
    "print('The best parameters searched are:', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regresssion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net regression uses a weighted average of the Lasso (L1) and Ridge (L2) penalties for regularization as shown in the formula below. \n",
    "\n",
    "$$\\hat{\\beta} = argmin\\left( \\left(y-X\\beta\\right)^2 + \\lambda*\\left(\\alpha*\\sum{\\left|\\beta\\right|} + \\left(1-\\alpha\\right)*\\sum{\\beta^2} \\right) \\right)$$\n",
    "\n",
    "Both penalties trade bias in the coefficient estimates, which are shrunk toward zero, for reduced model variance, which can improve predictive accuracy. \n",
    "\n",
    "When $\\alpha=1$ the penalty reduces to the usual Lasso (L1) penalty in terms of the absolute value of the coefficients. This has the effect that some coefficients are zeroed out or removed from the model, resulting in implicit feature selection. On the other hand $\\alpha=0$ results in the Ridge (L2) penalty. Since the L2 penalty is a function of the squared coefficients, the individual coefficients are shrunk toward zero but not typically zeroed out. \n",
    "\n",
    "Since the penalties are a function of the coefficients and these in turn are affected by the scale of the data, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters searched are: {'clf__alpha': 0.25, 'clf__l1_ratio': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "predPipeENET = Pipeline(steps = [('scale', StandardScaler()), ('clf', ElasticNet(random_state = rand_state))])\n",
    "\n",
    "# Note: that sklearn's alpha penalty is the same as lambda in R's glmnet package\n",
    "# thus, alpha controls strength of regularization\n",
    "# Note: sklearn's l1_ratio is same as alpha in glmnet and controls mixing of l1 and l2 penalties\n",
    "# l1_ratio = 1 runs straight Lasso, while l1_ratio = 0 runs straight ridge regression\n",
    "\n",
    "parameters = {'clf__alpha':[0.25, 0.5, 1, 5, 10, 100], 'clf__l1_ratio':[0,0.1, 0.25, 0.5, 0.75, 0.9, 1]}\n",
    "clf = GridSearchCV(predPipeENET, parameters, scoring = scorer, cv=5, n_jobs = -1)\n",
    "clf.fit(x_data, target)\n",
    "resultsENET = clf.cv_results_\n",
    "\n",
    "print('The best parameters searched are:', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fit a random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters searched are: {'max_depth': None, 'max_features': 8}\n"
     ]
    }
   ],
   "source": [
    "predPipeENET = Pipeline(steps = [('scale', StandardScaler()), ('clf', RandomForestRegressor(n_estimators = 500) )])\n",
    "\n",
    "parameters = {'max_features':[4, 8, 13], 'max_depth':[1, 5, 10, None],}\n",
    "rfReg = RandomForestRegressor(n_estimators = 500) \n",
    "clf = GridSearchCV(rfReg, parameters, scoring = scorer, cv=5)\n",
    "clf.fit(x_data, target)\n",
    "\n",
    "resultsRFReg = clf.cv_results_\n",
    "\n",
    "print('The best parameters searched are:', clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe gradient boosted regression here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
